#!/usr/bin/env python

from argparse import ArgumentParser
import os
import cfg
import sys
import multiprocessing
import time
dir = os.path.abspath(os.path.dirname(__file__))

parser = ArgumentParser()
d = cfg.parse_args(parser,config='$outdir/rules.config')
hp = d.hadoop

#print >> sys.stderr, d.config

def runit(x,y):
  n = 0
  while True:
    try:
        if n > 0:
            print >> sys.stderr, "rerunning runit"
        x.run(y)
        return 0
    except:
        if n <= 3:
            n += 1
            print >> sys.stderr, "WARNING. runit failed. retrying in 1 minute"
            time.sleep(60)
        else:
            raise


if os.path.exists(os.path.join(d.outdir,'rules.final')):
    hp.syscall('mv ' + os.path.join(d.outdir,'rules.final') + ' ' + d.tmpdir)
if not os.path.exists(os.path.join(d.tmpdir,'rules.final')):
    
    if not hp.file_exists('rules.extracted'):
        hp.put(os.path.join(d.tmpdir,'rules.extracted'),'rules')
    else:
        hp.move('rules.extracted','rules')
    steps = cfg.steps(d)
    joins = [ 'part.rules' ]

    # prefeature steps do not create data directly for features and must run
    # before features
    for s in steps:
        if s.stage == 'prefeature':
            s.run(hp)
    pool = multiprocessing.Pool()
    hp.mkdir('/tmp')
    results = []
    for s in steps:
        if s.stage == 'feature':
            #print >> sys.stderr, s
            results.append(pool.apply_async(runit,(s,hp)))
            joins.extend(s.output_filename())
    for r in results:
        r.get() # raise any exceptions from worker threads
    pool.close()
    pool.join()
    scriptdir = d.config['variables']['rhsdir']
    joiner = os.path.join(scriptdir,'join')
    jm = os.path.join(scriptdir,'paste','mapper')
    jr = os.path.join(scriptdir,'paste','reducer')
    
    hp.mapreduce( mapper=jm 
                , reducer='NONE'
                , input='rules'
                , output='part.rules' )
                
    hp.remove('rules')
    
    hp.syscall(joiner + ' ' + ' '.join(joins) + ' -e -E -c ' + d.config_files + ' -o rules.paste' )
    hp.mapreduce( mapper=jr
                , input='rules.paste'
                , output='rules.final'
                )
    hp.remove('rules.paste')
    hp.mapreduce( mapper=os.path.join(scriptdir,'wc','mapper')
                , reducer=os.path.join(scriptdir,'wc','reducer')
                , input='rules.final'
                , output='rules.final.wc' 
                , options='-jobconf mapred.output.compress=false'
                )
    hp.getmerge('rules.final.wc', os.path.join(d.outdir,'rules.final.wc'))
    hp.get('rules.final',os.path.join(d.tmpdir,'rules.final'))
    
    hp.remove('rules.final.wc')
    for part in joins:
        hp.remove(part)

