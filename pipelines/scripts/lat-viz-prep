#!/usr/bin/env perl 
#
# visualization based on Wei's script, using Python modules, and
# employing new tricks from the new decoder. This script reads one
# n-best decoder output, and the corresponding global gar file, and the
# per-sentence far or brf files.
#
# For each sentence, it produces three output files. The basename
# is the sentence's chunk position. The true position is part of 
# the output.
#
# REALID: real sentence number [foreign-length]
# 1BEST: [1]: original sentence (foreign)
#        [2]: 1-best translation (English)
#        [3]: derivation tree
#        [4]: rule tree
# FSDAG: decoder's input lattice
# RULES: xrs rules for the 1best hypothesis
# NBEST  decoder unique n-best hypotheses
#
use 5.006;
use strict;
use warnings;
use Getopt::Long qw(:config bundling no_ignore_case);
use File::Basename;
use File::Temp qw(tempfile tempdir);
use Cwd;
my $verbose = 1;
$main::start = time();
%main::unlink = ();
END { unlink keys %main::unlink if ( %main::unlink ) }

my $clean = 1;			# keep files: set to 0
my $tmpdir = $ENV{'MY_TMP'} || 
    $ENV{TMP} || 
    $ENV{TEMP} || 
    $ENV{TMPDIR} ||
    File::Spec->tmpdir() || 
    '/tmp';
$tmpdir = '/scratch' if -d '/scratch';

my ($grammar_view,%corpus_map); 
my $xsearchdb = '';
#
# --- functions -------------------------------------------------
#

sub trim($)
{
    my $string = shift;
    $string =~ s/^\s+//;
    $string =~ s/\s+$//;
    return $string;
}

sub cmap {
    my ($key,$val) = @_;
    open CMAP, $val or die $!;
    while (my $line = <CMAP>) {
	if (substr($line,0,1) ne  "#") {
	    my @v = split('\t',trim($line));
            print STDERR "$v[0] => $v[2]\n";
	    $corpus_map{$v[0]} = $v[2];
	}
    }
}

sub usage {
    print << "EOF";
Usage: @{[basename($0)]} [options] --map corpus.map n-best-file

Optional arguments:
 --verbose             more messages
 --global unpack.xrs   unpacked both-side non-lex (global.gar) rules. 
                       do not specify, if you don\'t use global grammars.

Mandatory arguments:
 --chunk i             number of this chunk (for parallel processing)
                       you should use as many chunks as decoding or tuning did.
 --grammar-view exe    location of the grammar_view application.
 --output fn           where to put the combined output files.
 --index fn            where to put the index for the output file.
 --per-sentence dir    base directory where to start searching for rules.
 --corpus-map file     where are all the sentence directories
EOF

    exit(1);
}

sub iso8601(;$) {
    # purpose: poor man's strftime (avoid loading POSIX)
    # returns: ISO 8601 like (not quite) formatted date stamp space time stamp
    # warning: somewhat fudges the standard concerning zone and separator
    #
    my @x = localtime( defined $_[0] ? shift() : time() );
    sprintf( "%04u-%02u-%02u %02u:%02u:%02u",
	     $x[5]+1900, $x[4]+1, @x[3,2,1,0] );
}

sub format_e($) {
    my $x = shift;
    my $f = $x < 1 ? '=%0.4g' : '=%0.4f';
    sprintf $f, exp(1) ** $x;
}

sub format_10($) {
    my $x = shift;
    my $f = $x < 1 ? '=%0.4g' : '=%0.4f';
    # sprintf $f, 10.0 ** $x;
    $x < 0 ? ( '=' . substr($x,1) ) : sprintf( $f, -$x );
    ## sprintf $f, -$x;
}

sub unpacked_rules(*$$@) {
    # purpose: extract rules-ids for a sentence. uniq applied internally.
    # paramtr: *HYP (IO): file handle open for writing
    #          $sentence (IN): sentence number
    #          $sdir (IN): per sentence grammar directory 
    #          @ruleids (IN): array of rule ids
    #
    local(*NEW) = shift;
    my $sentence = shift;
    my $sdir = shift; 
    my %ruleids = map { $_ => 1 } @_; 

    # extract global rules 
    my %result = (); 
    my $m = 0;			# global rule count
    my $mm = 0;
 
    #
    # call grammar view with remainig rule ids
    #
    warn "# @{[iso8601]}: Extracting rules for sentence $sentence\n";

    my $arg = "cat $sdir/decode.ins | $grammar_view $xsearchdb --rule-dump";

    $arg .= ' 2>>/dev/null' unless $verbose > 1;

    warn "# $arg\n" if $verbose;
    my $n = 0;			# local rule count
    my $nn = 0;			# total local rule
    if ( open( GVIEW, "$arg|" ) ) {
	while ( <GVIEW> ) {
	    chomp; 
	    if ( / id=(-?\d+)/ ) {
		my $ruleid = $1; 
		++$nn; 
		next unless exists $ruleids{$ruleid}; 
		warn "# Found local rule $ruleid\n" if $verbose > 1; 

		# convert-e^.py
		s/=e\^(\S+)/format_e($1)/eg;
		s/=10\^(\S+)/format_10($1)/eg;
		$result{$ruleid} = $_; 
		++$n; 
	    }
	}
	close GVIEW || die "ERROR: close $arg: $!\n"; 
    } else {
	die "ERROR: popen $arg: $!\n"; 
    }
    warn "# Found $n/$nn local rules\n"; 

    # print all found ruleids
    foreach my $id ( keys %result ) {
	delete $ruleids{$id}; 
	print NEW "RULES: $result{$id}\n";
    }
    warn( "# @{[iso8601]}: found $m+$n=", $m+$n, 
	  " matches in $mm+$nn=", $mm+$nn, " rules\n" );

    if ( %ruleids ) {
	warn( "Warning: Failed to find the following rules: ",
	      join(' ',keys %ruleids), "\n" );
    }
}

sub generate_1best(*$$$) {
    # purpose: Emulate generate-1best.py 
    # paramtr: HYP (IO): filehandle opened for writing
    #          $real_sentence (IN): number of real sentence
    #          $sdir (IN): where to find sentence stuff. 
    #          $line (IN): 1-best hypothesis line from n-best file
    #
    local (*BEST) = shift;
    my $sentence = shift;
    my $sdir = shift; 
    local $_ = shift;
    local(*S,*DAG);
    print STDERR "sent=$sentence sdir=$sdir\n"; 

    my $ffn = File::Spec->catfile( $sdir, 'sentence' ); 
    if ( open( S, "<$ffn" ) ) {
	my $x = <S>; 
	close S; 
	$x =~ s{^<foreign-sentence> }{};
        $x =~ s{^</foreign-sentence> }{};
	$x =~ s{[\r\n]+$}{}; 	# chomp
	my @x = split ' ', $x;

	# generator NEEDS the foreign length!
	print BEST "REALID: $sentence ", @x+0, "\n";
	print BEST "1BEST: $x\n";
    } else {
	die "FATAL: Unable to determine foreign plain sentence"; 
    }

    if ( m/\shyp=\{\{\{(.*?)\}\}\}/ ) {
	print BEST "1BEST: $1\n";
    } else {
	die "FATAL: Unable to determine attr.hyp";
    }

    if ( m/\stree=\{\{\{(.*?)\}\}\}/ ) {
	print BEST "1BEST: $1\n";
    } else {
	die "FATAL: Unable to determine attr.tree";
    }

    if ( m/\sderivation=\{\{\{(.*?)\}\}\}/ ) {
	my $x = $1;
	$x =~ s/\(\s*\)//g;	# only literal open-close
	$x =~ s/^\s*//;		# trim front
	$x =~ s/\s*$//;		# trim rear
	$x =~ s/\s+/ /g;	# squeeze multiple whitespaces
        $x =~ s/\[[^\s]*\]<[^\s]*>//g;
	print BEST "1BEST: $x\n";
    } else {
	die "FATAL: Unable to determine attr.derivation";
    }

    # store decoder's input lattice
    my $latfn = File::Spec->catfile( $sdir, 'lattice' );
    if ( open( DAG, "<$latfn" ) ) {
	local $/;		# scoped slurp mode
	my $x = <DAG>;
	chomp($x); 
	close DAG; 
	$x =~ s{\s+}{ }g;	# make it a single line
	print HYP "FSDAG: $x\n"; 
    } else {
	warn "# Unable to find lattice for sentence $sentence in $latfn\n";
    }
}

sub prepare_viz_dat($$$) {
    # purpose: extract n-best lists from n-best file
    # paramtr: $fn (IN): new decoder's n-best separate output
    #          $chunk (IN): which chunk number
    #          %global (IN): unpacked both-side non-lex global.gar file (or undef)
    #          $basedir (IN): where to start looking for grammars
    #          $indexfn (IN): file name for sentence indices for this chunk
    #          $outfn (IN): file name for cooked output for this chunk
    # returns: -
    #
    my ($basedir,$indexfn,$outfn) = @_; 
    local (*HYP,*IDX,*DAG);

    #
    # open output files
    #
    open( IDX, ">$indexfn" ) || die "ERROR: open indexfn $indexfn: $!\n";
    open( HYP, ">$outfn" ) || die "ERROR: open outfn $outfn: $!\n";
    #binmode( HYP, ':utf8' ) || die "ERROR: binmode utf8: $!\n";

    my ($sentence,$nbest,$hypothesis,%hypothesis,$rulefn,$hypfn,$bestfn);
    my $prev_sent_no = 0;	# sentences start at 1
    my $sent_count = 0; 	# sentence count
    while ( <> ) {
	# new decoder does not have recovered hypotheses any more
	my $fix = substr( $_, 0, 32 );
	if ( $fix =~ /^NBEST sent=(\d+) nbest=(\d+)/ ) {
	    ($sentence,$nbest) = ($1,$2);
	    my $save = $_;
	    my $aa = sprintf "%02d", int($sentence / 100);
	    my $bb = sprintf "%02d", int($sentence % 100); 
	    my $sdir = File::Spec->catfile( $basedir, $corpus_map{$sentence} ); 

	    if ( $prev_sent_no != $sentence ) {
		# new sentence
		if ( $prev_sent_no != 0 ) {
		    print IDX "\t", tell(HYP), "\n";
		    print HYP "\n";
		}
		
		# output current sentence
		print IDX $sentence, "\t", tell(HYP);
		## deferred to generate_1best for now!!!
		## print HYP "REALID: $sentence\n";

		%hypothesis = ();
		$prev_sent_no = $sentence;
		++$sent_count; 
		warn "\n# @{[iso8601]}: [$sent_count] Processing sentence $sentence\n";

		# produce intermediary file for later consumption
		generate_1best( HYP, $sentence, $sdir, $save );

		# save 1-best derivation's rules
		$_ = $save;
		if ( /\sderivation=\{\{\{(.*?)\}\}\}/ ) {
		    my $x = $1;
		    next unless length $x;
		    $x =~ s/[\(\)]//g;
		    $x =~ s/^\s*//; # trim front
		    $x =~ s/\s*$//; # trim rear
                    $x =~ s/\[[^\s]*\]<[^\s]*>//g;
		    # FIXME: Can we just drop UNK rules? 
		    $x =~ s/UNK/-88888888/g; 
		    my @x = sort { $a <=> $b } 
		            keys %{{ map { $_ => 1 } split /\s/, $x }}; 

		    warn( "# Seeking ", @x+0, " 1-best rules: @x\n" ) if $verbose; 
		    $rulefn = unpacked_rules( HYP, $sentence, $sdir, @x ); 
		} else {
		    warn "Warning: No rules found for sentence $sentence\n";
		}
	    }

	    # collect unique hypothesis n-best, while we are at it
	    $_ = $save;
	    if ( m<\shyp=\{\{\{(.*?)\}\}\}> ) {
		$hypothesis = $1;
		print HYP $save unless exists $hypothesis{$hypothesis};
		$hypothesis{$hypothesis} = 1;
	    } else {
		# no hypothesis, what is going on?
		warn "Warning: No hypothesis for $sentence:$nbest\n";
	    }
	} else {
	    # unrecognized input
	    print STDERR $_ if $verbose > 1;
	}
    }


    if ( $prev_sent_no != 0 ) {
	print IDX "\t", tell(HYP), "\n";
	print HYP "\n";
    }

    close IDX;
    close HYP;
}

#
# --- main ------------------------------------------------------
#
usage unless @ARGV;
warn "# @{[iso8601]}: $0 @ARGV\n";

$SIG{INT} = sub { exit 1; };

my ($per_sentence);
my $chunk = 0;
my $outfn = '-';
my $indexfn = File::Spec->devnull();
GetOptions( 'help|h' => \&usage
	  , 'verbose|v!' => \$verbose
	  , 'map=s' => sub { 
	      # noop
	      warn "Warning: Ignoring --map option for now\n"; 
	  },	 
	  , 'xsearchdb=s' => \$xsearchdb
	  , 'per-sentence=s' => \$per_sentence
	  , 'grammar-view=s' => \$grammar_view
          , 'corpus-map=s' => \&cmap
	  , 'output=s' => \$outfn
	  , 'index=s' => \$indexfn
	  ) || 
    die "ERROR: Problem processing options\n";

chdir $per_sentence;

if ( defined $per_sentence && length($per_sentence) ) {
    die "ERROR: Unable to read --per-sentence location\n"
	unless ( -d $per_sentence || -f _ );

    die "ERROR: grammar_view does not exist\n" 
	unless ( defined $grammar_view && -r $grammar_view );
    die "ERROR: grammar_view is not executable\n" 
	unless ( -x _ );
} else {
    die "ERROR: Must specify a --per-sentence directory\n"; 
}





warn "# @{[iso8601]}: starting\n\n" if $verbose;
prepare_viz_dat($per_sentence,$indexfn,$outfn);
my $diff = time() - $main::start;
warn "\n# @{[iso8601]}: finished, $diff s\n" if $verbose;

exit 0;
