#!/usr/bin/env python

# adapted from david chiangs join.  avoids use of hadoop-specific binary features, only using 
# hadoop.py to interface with hadoop.

import hadoop
import cfg
import argparse
import os.path
import sys
import math
import itertools

def crossproduct(*xs,**kw):
    empty = False
    if 'empty' in kw:
        empty = kw['empty']
    #print >> sys.stderr, 'crossproduct: len(xs) ==', len(xs)
    if len(xs) == 0:
        if empty:
            yield []
        return
    elif len(xs) == 1:
        if len(xs[0]) == 0 and empty:
            yield []
            return
        for x in xs[0]:
            yield x
        return
    else:
        if len(xs[0]) == 0 and empty:
            for ys in crossproduct(*xs[1:],**{'empty':empty}):
                yield ys
        else:
            for x in xs[0]:
                for ys in crossproduct(*xs[1:],**{'empty':empty}):
                    yield x+ys
                
def mapper(input,id,numkeys):
    for line in input:
        tbl = line.rstrip().split('\t')
        tbl[numkeys:numkeys] = [id]
        print '\t'.join(tbl)

def reducer(inp,numkeys,numsources,empty=False):
    print >> sys.stderr, 'empty=%s' % empty
    def input(inp):
        for line in inp:
            yield line.rstrip().split("\t")
    for key, records1 in itertools.groupby(input(inp), lambda fields: fields[:numkeys]):
        values = []
        done = False
        for source, records in itertools.groupby(records1, lambda fields: fields[numkeys]):
            source = int(source)

            if source < numsources-1: # little sources
                assert not done
                while source > len(values)-1:
                    values.append([])

                for fields in records:
                    value = fields[numkeys+1:]
                    values[source].append(value)

            else: # big source
                #print >> sys.stderr, 'first len(values)=%s' % len(values)
                values = list(crossproduct(*values,empty=empty))
                #print >> sys.stderr, 'len(values)=%s, key=%s' % (len(values),key)
                for fields in records:
                    big_value = fields[numkeys+1:]
                    for value in values:
                        print "\t".join(key+big_value+value)
                    if len(values) == 0 and empty:
                        print "\t".join(key+big_value)
                done = True

        #if not done:
        #            raise Exception("no value from first source for key %s\n" % key)

parser = argparse.ArgumentParser()
parser.add_argument('input',nargs='*')
parser.add_argument('-e', '--empty', action='store_true')
parser.add_argument('-k','--numkeys', default=1, type=int)
parser.add_argument('-M','--map', dest='fileno')
parser.add_argument('-R','--reduce',type=int)
parser.add_argument('-r','--reducer')
parser.add_argument('-o','--output', default='join.out')
parser.add_argument('-c','--config')
parser.add_argument('-E','--erase', action='store_true')
d = parser.parse_args()

#print >> sys.stderr, 'main -e=%s' % d.empty

exe = os.path.abspath(__file__)
if d.fileno:
    mapper(sys.stdin,d.fileno,d.numkeys)
elif d.reduce:
    reducer(sys.stdin,d.numkeys,d.reduce,empty=d.empty)
else:
    config = cfg.load_config(d.config)
    hcfg = config['rule-extraction']['hadoop']
    hp = hadoop.Hadoop(echo=hcfg['echo'],serial=hcfg['serial'],home=hcfg['home'])
    if hp.file_exists(d.output):
        hp.remove(d.output)
    n = len(d.input)
    joininput = []
    for x,inp in enumerate(d.input):
        x = (x-1) % n
        dg = int(math.ceil(math.log10(n)))
        x = "%0*d" % (dg, x)
        jinp = '%s.join' % inp
        joininput.append(jinp)
        hp.mapreduce( input=inp
                    , output=jinp
                    , mapper='%s -M %s -k %s' % (exe,x,d.numkeys) 
                    , reducer='NONE' )
        if d.erase:
            hp.remove(inp)
    rdc = '%s -R %s -k %s' % (exe,n,d.numkeys)
    if d.empty:
        rdc += ' -e'
    hp.mapreduce( input=joininput
                , output=d.output + '.join.tmp' 
                , mapper='/bin/cat'
                , reducer=rdc
                , sortkeys=d.numkeys+1
                , partitionkeys=d.numkeys)
    
    for jinp in joininput:
        hp.remove(jinp)
    
    if d.reducer:
        hp.mapreduce( input=d.output + '.join.tmp' 
                    , output=d.output 
                    , mapper='/bin/cat'
                    , reducer=d.reducer
                    , sortkeys=d.numkeys)
        hp.remove(d.output + '.join.tmp')
    else:
        hp.move(d.output + '.join.tmp', d.output)

    

