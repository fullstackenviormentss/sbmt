% Rule Context Scoring
% Jonathan Graehl
% Jan 14, 2010

# Notation

$x$
  : a rule
$N$
  : the root tag of $x$
$f$
  : a foreign sentence under translation $f_1,\ldots,f_F$
$s=(a,b)$
  : the foreign span $f_a,\ldots,f_b$
$e$
  : $e_1,\ldots,e_E$, the English translation of $f_a,\ldots,f_b$ using the rule

# Features

In general we want to score a rule application in context: $p(x|f,(a,b),e,N)$ (caveat: $e$ depends on $x$), rather than the regular joint $p(x|N)$.  In practice we have a weighted sum of several independent features, some of which predict in the opposite direction e.g. $p(f|x)$.

## Non-contextual features

The original smoothed joint $p(x|N)$ is supplemented with hundreds of features by now, e.g. model1 and inverse model1.

## Global (whole foreign sentence) model1
$g(x,f) = \prod_{e\in x} p(e|NULL)+\sum_{i=1}^F p(e|f_i)$ where $p$ is the model1 t-table probability

## Context Language Model (Shen et. al)
$clm(f,(a,b),e) = p_{L}(e_1,e_0,f_{a-1}) p_{R}(e_{E-1},e_E,f_{b+1})$
where $p_{L}(a,b,c)$ is a trigram $p(c|a,b)$ learned from seeing foreign $c$ immediately preceding what has the English translation $(b,a,\ldots)$ (`<foreign-sentence>` and `</foreign-sentence>` are imagined as $f_0$ and $f_{F+1}$), and $p_{R}(x,y,z)$ is from foreign $z$ immediately following English translation $(\ldots,x,y)$.  If the English translation is just one word, then a bigram is used.

## Heuristic
Someone tried using a heuristic $h(N,f,(a,b))=h_{L}(f_{a-1}|N) h_{R}(f_{b+1}|N)$ a very long time ago.  It wasn't helpful in reducing search error.  But it wasn't tried as part of the model score.

## Syntactic context language model
$sclm(x,f,(a,b),e) = s_{L}(x,e_1,e_0,N,f_{a-1}) s_{R}(x,e_{E-1},e_E,N,f_{b+1})$, similar to context language model but with 5grams.  The regular ngram backoff rules imply that if a nonterminal $N$ wasn't seen in training with $e_0$ as its leftmost word and $f_{a-1}$ as the left-adjacent foreign word, then $s_{L}(x,e_1,e_0,N,f_{a-1})$ = $bo_{L}(x,e_1,e_0,N)*s_{L}(f_{a-1}|N)$.  Specific distributions for most rules would probably be sparse enough that a backoff would be used.  A less linear backoff scheme would require stepping outside the default ngram lm implementation.  For example, if $e_1,e_0,f_{a-1}$ is seen but not with the $N$, the standard backoff must be $e_0,N,f_{a-1}$.  Both sclm and clm would have some separate value if the sclm backoff scheme isn't flexible enough.

## Directly predict x given left and right context
The appeal of predicting left and right outside foreign words is that separate left and right context predictions can be multiplied together.  But we want $p(x|f,(a,b),e,N)$, so we could use ngram probabilities $p_L(x|e_1,e_0,N,f_{a-1}$ and $p_R(x|e_{E-1},e_E,N,f_{b+1}$.  But the regular ngram backoff scheme is likely way too rigid.  Alternatively, just fire off many indicator features (or different-backoff ngram models) for subsets of that context and discriminatively learn them.  Can we collect counts over training and use those as strong priors for discriminative training?

## xrs rule clustering
Learn a hard clustering $C=class(x)$ of xrs rules (other than the obvious root $N$).  Then include $C$ in the $sclm$ or $p_L$ and $p_R$ ngrams.

# Binarized Rule Scoring

Maybe we just score xrs rules.  But we parse using binarized rules and prefer to score every item, not just those that complete an xrs rule.  For composed rules, if we have available the subbrackets of the minimal rules making them up, we can score every minimal rule bracket.

Global model1 is only scored for xrs rules, but because it's context independent (given the input sentence) it's just a regular rule feature and contributes a meaningful score (via admissible heuristic) to each binary item.

Context language model events could be computed for binary edges as well (because our xrs rules are itg-binarized into binary rules that always cover a contiguous e-span).  However, ngram counts would need to be collected over binarized brackets during rule extraction.  Since during rule acquisition we haven't yet decided on a binarization, I'd either emit clm ngram events for all possible binarizations, or postpone the work until I can reconcile the xrs derivation forest with the binarized rules.

In the case of the syntactic clm, the events for many virtual nonterminals would be quite sparse.  A simpler option would be to score virtual (non-xrs binary) items with the non-syntactic clm statistic: $p_{L}(e_1,e_0,f_{a-1}$), or, as always, to just score xrs rules.

# Competition between small and big rules

If our only features were properly normalized $p(x|f,(a,b),N)$, then small and big (especially composed) rules could compete sensibly.  For any translation (derivation) of the entire $f$, there is an xrs-parse with multiple (or no) left and right brackets adjacent each foreign word, and similarly for a binary-parse.  clm and sclm will then predict ($p_{L}(f_i|\ldots)$) a different multiset of foreign words depending on the bracketing.  For xrs-only clm, larger rules will predict fewer words, which will reward them beyond the usual benefit of having memorized a large chunk of training data if we didn't smooth perfectly.  So we might want to reward smaller rules (or even all rules), e.g. through a constant reward feature on each xrs rule (we have this already in `derivation-size`).

