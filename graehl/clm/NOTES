Jonathan Graehl wrote:
> I have another question:
>
> Do your rules ever have more than 2 variables?  Ours can, and when they

No, we have up to two variables.

> do, we binarize them so that they have <= 2.  So far I've only scored the outside foreign words for the span the whole rule is over, but I feel like I probably should do that for the spans for each of the binarized rules.  This would require that my lm training know about the binarization of rules as well (we don't just go left->right).
>
> What do you do with unknown foreign words?  I trained (using sri tools) a lm with a p(<unk>), but given that only some of the foreign words are predicted, it seems like derivations with fewer brackets adjacent the unknown word are rewarded:
>
> ((((a b) c) U) (d (e f)))
>
> vs.
>
> (((a (b c)) U) ((d e) f))
>
> where U is a foreign word (maybe a named entity) we didn't see in the training data.

I haven't thought about the case of unknown words before. Yes, you are right. It would prefer one splitting than another based on the position of unknown words. In fact, the context LM is mainly designed to disambiguate hypotheses with the same splitting. I once even wanted to treat it as an outside estimate instead of part of the inside score, but I didn't try it.


=============

* unique-per-location (extract -X | sort | uniq) counts can be obtained with minimal rules.

not necessary: p(f<s>|e<s> e1) because p(e1|e<s>) in monolingual LM already captures this entirely and p (f<s>|e<s>) = 1.  similar for p(f</s>|ek </s>).


> Finally, I'll note that I'm supplying only max-order counts to SRI's ngram-count -read counts-file; it seems to write an lm with lower-order counts filled in for me, which is nice.  This is safe to do, right?
>

WRONG! - this is true only for unigrams; singleton trigram "Efrom Eapart F<s>" didn't include any Eapart in the trained lm unless I also included an explicit count for bigram "Eapart F<s>".

=============

I would prefer to collect events on the basis of the binarized-xrs-rule ghkm forests (we never really create them, but conceptually they should exist), so I can safely score virtual edges in the decoder.  I can certainly score them on the basis of statistics gathered only on whole-xrs-rules, and probably get a decent approximation, but the right thing is surely to figure out, after xrs-binarization, what additional sentence/espan/fspan are induced.

I don't see anything stopping me from scoring every item in decoding on the basis of the single-per-location counts I'm using now.

=============

I'm concerned that by predicting outside f words, derivations like (f0 (((f1 f2) f3) f4)) will predict many times the same f word, e.g. f0 would be the outside word for the three consecutive brackets (((f1.

So, the derivation-size feature or similar will have to take care of this difference.

==============

Also, I assume that you predict foreign-sentence <s> and </s> boundaries?

Libin Shen wrote:
> Jonathan,
>
> I never I thought about it. We use bigrams only at BBN.
> Libin
>
> On Fri, 4 Dec 2009, Jonathan Graehl wrote:
>
>
>> When there's a single-english-word translation rule in context:
>>
>> f1 (f2 f3 -> e2) f4
>>
>> Should we learn (and look up in decoding) trigrams P_R(f4|e2,f1) and P_L(f1|e2,f4), trigrams P_R(f4|e2,_) and P_L(f1|e2,_) (treating this differently from a backoff to bigram, preferring first to use statistics from single-e-word-spans), or just backed off bigrams P_R(f4|e2) and P_L(f1|e2)?
>>
>>

================

Libin Shen wrote:
> Hi Jonathan,
>
> In my original implementation, I had certain level of replication of the same event in a sentence. The event extraction procedure is exactly the same as hierdec rule extraction procedure. However, I limited it to at most one NT for each rule for event extraction. Later, someone here copied my sequence, and allowed two NTs for each rule for event extraction, which resulted in more replication, but we observed similar results, and we kept the latter setup.
>
So it's possible that you have an ngram that occurs only in a single sentence pair, but you end up with count >1.  Would you say that it's possible that such an ngram could occur  as many as 5 or 10 times in a single sentence?  For example, consider the very leftmost bracketing (((((e1 e2) (e3 e4)) e5) e6)), and assume the left ((((( all get the same foreign left-span.

We actually have a similar flaw in our current system for assigning rule probabilities that we haven't gotten around to correcting, and I also don't think it's hurting our overall system much.
> As to WB smoothing, it's because I got some problem when training a KN model with SRI's LM toolkit. The n-grams observed here are different from what we have for regular string n-grams, because the prefixes of the English side do not occur as events. KN smoothing has a problem with that, so I switch to WB smoothing.
>
That's good to know; I was going to use SRI's tools.  Thanks for the tip.

-Jonathan
> Libin
>
> Spyros Matsoukas wrote:
>> Hi Jonathan,
>>
>>   Libin is the one who actually implemented this context LM, so
>> he is the best person to answer your question.
>>
>> Libin, could you please describe how we count for the context LM?
>>
>> Thanks,
>>
>> --Spyros
>>
>>
>>
>> Jonathan Graehl wrote:
>>
>>> In your paper "Effective Use of Linguistic and Contextual Information for SMT", you say that "when we extract a rule, we collect two 3-gram events, one for the left side [and the other for the right]".  Do you remove duplicate counts caused by the same location in the training data?
>>>
>>> I'll elaborate.  For simplicity, I'll just talk about the left-side events in what follows; the same would naturally be done, independently, for the right-side events.
>>>
>>> When we extract rules from our aligned and parsed parallel data, we may generate many rules for the same ((p,q),(i,j)) (e-span,f-span) pair, and even more where just the left (p,i) (english,foreign) span boundaries are the same. So, I plan to either collect at most one count for a given left event (f_{i-1}|e_p,e_{p+1}) per sentence, or, with a little more effort, give one count to the events resulting from the unique left (p,i) positions arising in rule extraction, since technically it's more correct to give multiple counts in the rare event that the same ngram occurs in multiple (p,i) positions.
>>>
>>> Alternatively, I could collect weighted fractional counts based on the portion of derivation trees in our extracted rule forest where each (p,i) position occurs.
>>>
>>> What approach did you use in combining redundant events resulting from rule extraction?  It may be that your rule extraction process has less redundancy than ours.
>>>
>>> Finally, I'm curious why you used Witten-Bell discounting as opposed to say, modified Kneser-Ney (the latter is supposed to be give imperceptibly better perplexity in monolingual language models).

