It would be great to see plots where there is a tighter relationship between x and y axes. For example

x = Pcache of a sample (not just the last)
y = accuracy of sample (not viterbi)

x = average log Pcache of all samples from one run = entropy
y = average accuracy of samples

Hypothesis: minimum entropy is a good way to choose a run. It suggests that the sampler has wandered into the good part of the space. That's my interpretation of your plot on p. 3, upper right.

x = accuracy of viterbi decoding trained from all samples
y = average accuracy of samples

Another decision rule not discussed yet, but which gives good accuracies, is to take the tagging obtained by, for each word, taking the tag that occurs in the most samples.

=====


derivations graph.h has data as int index into an arcs_table<train.h::arc_counts_base::arc (FSTArc *)+em_weight,counts,etc.>

for gibbs, just the .arc *?

.arc->groupId gives cascade chain index

=======

More efficient train-cascade + -a (one param per arc) - we like writing the trained transducers back, but can change slist chain of real param pointer to just point to original arc?  Could simplify gibbs to require -a as well?

==========

A bummer of a wart in gibbs w/ a single transducer: --train-cascade, which is mandatory for gibbs, doesn't create the same indirect id->param list structure; it's special cased for trivial (non-cascade) case.  can we build a real singleton cascade for gibbs, or do we need a special case of all the cascade operations used in gibbs?  (latter should be more efficient).  plan for special case: instead of path = pointers to param chains, path = pointers to fst arc (do it by casting or union?)

===========

What's wrong with current cascade+derivation for gibbs?

In derivation, an arcs_table<arc_counts> has priors, counts, etc. per composed arc.  The derivations are saved as a graph with .data_as<unsigned> = index into table.  But in a cascade, each graph arc may be more than one input fst arc.  So forward/backward/random happen at the per-composed-arc level.

Composed arcs do have (tie) groups which index into cascade; same group = same parameter.  Counts are summed then normalized

Cascade takes composed arc counts and adds them to each label in composed arcs' cascade chain, then copies summed counts to original chain arc weight field, then asks chain to normalize itself

For gibbs, we need to replace or supplement the composed arcs' group->chain label "list of original arc*" with "list of (arc *,normgroup id,count,...).  We can mandate -a so lists are all trivial (0 or 1 long).  The original arcs right now have their groups replaced w/ consec. indices WFST::set_normgroups.  Indirection could be done there so each orig arc gets its own "gibbs state=normgroup id,count" index, instead of changing cascade type (affecting composition operations) when doing gibbs.

=================

A cascade of N WFST are composed, giving WFST C.  Weights of the original WFST are adjusted subject to normalization (conditional or joint WFST::NormalizeMethod).  Then a sequence of (input,output) sentences are explained using the composition.  In a single EM training iteration, expected counts are computed for each arc in the composition, and using cascade.h, the counts are assigned to the original WFST, and normalized.

Some options are abstracted.  Usually a single class type handles the abstraction, with members/functions for the alternatives contained in it.  The unused members are designed to occupy only trivial space.

For instance:

cascade may have 1 WFST; then cascade_parameters.trivial == true.

cascades of more than 1 thing mean that the composition tracks linked lists ("chains") of original WFST parameters for each arc in C.  to do training of the cascade, the "--train-cascade" option must be used, otherwise the result of composition is used a singleton to be trained.  cascade.h then handles transfer of counts from arcs in C to their chains' origins, normalization of the N cascade WFST, and regeneration of new weights in C from the chains. Also, temporary storage for the "global best" weights in the N original WFST.

another option: "-a" modifies composition so that each arc in C has a singleton chain.  this is often more efficient (even though it creates more states, C may have many fewer arcs!).  See compose.cc

"--disk-cache-derivations" or "-?" (memory cache derivations) will compute precisely the intersection of each (input,output) example with C, prune it, and save it.  Then expected counts are computed over that structure in linear time.  Without caching, the full matrix of forward/backward for each (input-suffix,C-state,output-suffix) is generated for each example at each iteration, which may be slower in sparse structures.  See derivations.{h,cc} and train.cc:forward_backward.use_matrix

derivations.h has its own graehl/shared/graph.h based representation of the intersections of C with examples.  This intersection could have been stored as a WFST, but the graph structure is more compact.

---+ Proposed blocked Gibbs sampling (Bayesian prior) implementation

Require cache-derivations for efficiency (matrix is deprecated).

New inputs:

# of samples per example per iteration: probably just 1.  as # approaches infinity, you'd just have forward/backward frac. counts

Priors: Each normalization group in the N WFSTs may have a "base model" uniform prior beta, so that you have a proposal HMM prob of p[i]=(beta+count[i])/Z.  Perhaps one beta per WFST until an input format for annotating the N input WFST's normgroups' priors (or a separate description file for named states: (state,input) or (state) => prior).  This can be done using the cascade chains to read the original arcs from input WFST #1.

Base model (p0) from initial cascade xdcr weights?  What's the collapsed proposal HMM in that case?

Optional unsupervised decode (deterministic annealing schedule of start/end temperature).  Show the outputs of the leftmost cascade WFST (usually *e*:HIDDEN is used and (input,output) examples all have input=*e*) corresponding to the final sample.

Trained weights collection: burn-in (initially skip B) and epoch (collect 1, skip E,repeat) for cumulative counts.  Perhaps trained weights output can be disabled entirely for unsupervised decode.


---- maintaining past/future counts / probs

Normalizing cascade parameters: it's important to not recompute full sum-of-counts each time we move to another example.  Therefore, associated with each arc in cascade: norm-group-id(pointer to shared Z sum) and count. Both counts include prior and exclude the current example.

Therefore we also need to save the counts for previous iteration for each example.  If # samples per example = 1 then could just store the particular derivation chosen, and subtract its counts out.  We store the derivation chosen so we can print --unsupervised hidden sequence, too.  Any time we take out counts, we take them out from arc and norm-Z-sums.

Need then to iterate using Normalize type (cond/joint) to identify derivation graph arc indices with normgroup (meaning -a mandatory, probably a good idea),
OR:
cascade tracks FSTArc * (<graehl/shared/arc.h> - weird, would think that would go in carmel/src).  steal group ids except locked id when doing gibbs?  hash? change cascade rep to use integer arc table index instead?

In sampling, we won't actually need to push weights back to original cascade except at very end if we choose to write trained results.